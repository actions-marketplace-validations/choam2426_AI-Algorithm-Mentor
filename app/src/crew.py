from textwrap import dedent

from crewai import LLM, Agent, Crew, Process, Task

from .config import LLMConfig
from .consts import LLMProvider


def get_crewai_llm(llm_config: LLMConfig) -> LLM:
    """
    config.py ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ CrewAIì˜ LLM ê°ì²´(LiteLLM ê¸°ë°˜)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    LiteLLMì€ ê³µê¸‰ìž(Provider)ì— ë”°ë¼ ëª¨ë¸ëª…ì— ì ‘ë‘ì‚¬ê°€ í•„ìš”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    """
    model_name = llm_config.model_name

    # LiteLLM ëª¨ë¸ ì´ë¦„ ê·œì¹™ ì ìš© (provider/model_name)
    if llm_config.provider == LLMProvider.ANTHROPIC:
        if not model_name.startswith("anthropic/"):
            model_name = f"anthropic/{model_name}"
    elif llm_config.provider == LLMProvider.GOOGLE:
        if not model_name.startswith("gemini/"):
            model_name = f"gemini/{model_name}"
    elif llm_config.provider == LLMProvider.OPENAI:
        # OpenAIëŠ” ì ‘ë‘ì‚¬ ì—†ì´ë„ ë™ìž‘í•˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ openai/ë¥¼ ë¶™ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì‚¬ìš©ìžê°€ í™˜ê²½ ë³€ìˆ˜ì— ìž…ë ¥í•œ ëª¨ë¸ëª…ì„ ìš°ì„ í•©ë‹ˆë‹¤.
        pass

    # CrewAIì˜ LLM í´ëž˜ìŠ¤ëŠ” LiteLLMì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    # API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜(OPENAI_API_KEY, ANTHROPIC_API_KEY ë“±)ì—ì„œ ìžë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.
    return LLM(model=model_name)


class AlgorithmReviewCrew:
    def __init__(self, problem_info: str, solution_code: str, llm_config: LLMConfig):
        self.problem_info = problem_info
        self.solution_code = solution_code
        self.llm_config = llm_config
        self.llm = get_crewai_llm(llm_config)

    def logic_expert(self) -> Agent:
        return Agent(
            role="Algorithm Logic Verifier",
            goal="Verify the correctness of the solution against the problem requirements.",
            backstory=dedent("""
                You are a strict Algorithm Judge. Your sole focus is correctness.
                You check if the logic holds for all edge cases, boundary conditions, and potential constraints.
                You are modeled after rigorous Online Judge systems (BOJ, Codeforces).
            """),
            llm=self.llm,
            verbose=True,
            allow_delegation=False,
        )

    def performance_specialist(self) -> Agent:
        return Agent(
            role="Performance & Complexity Analyst",
            goal="Analyze Time and Space complexity and suggest optimizations.",
            backstory=dedent("""
                You are an Optimization Guru. You care about Big-O notation.
                You despise inefficient loops and redundant calculations.
                You look for the most optimal data structures and algorithmic approaches.
            """),
            llm=self.llm,
            verbose=True,
            allow_delegation=False,
        )

    def code_quality_mentor(self) -> Agent:
        return Agent(
            role="Clean Code Mentor",
            goal="Ensure code readability, proper naming, and maintainability.",
            backstory=dedent("""
                You are a Senior Software Engineer who values clean, Pythonic (or language-idiomatic) code.
                You look for bad variable names, lack of modularity, and messy structures.
                You want the code to be readable by humans, not just computers.
            """),
            llm=self.llm,
            verbose=True,
            allow_delegation=False,
        )

    def review_task(self, agent: Agent, focus_area: str) -> Task:
        return Task(
            description=dedent(f"""
                Analyze the provided solution code for the given problem.
                
                [Problem Info]
                {self.problem_info}

                [Solution Code]
                {self.solution_code}

                Your focus is: {focus_area}
                Provide a detailed report on your findings strictly related to your role.
            """),
            expected_output=dedent(f"""
                A structured section focused on {focus_area}.
                Include specific examples from the code and actionable suggestions.
            """),
            agent=agent,
        )

    def report_aggregator_task(self, agent: Agent, context: list[Task]) -> Task:
        return Task(
            description=dedent(f"""
                Synthesize the findings from the Logic Verifier, Performance Analyst, and Code Mentor.
                Create a final, comprehensive Markdown report in {self.llm_config.response_language}.
                The report should be encouraging but technically rigorous.
            """),
            expected_output=dedent("""
                A final Markdown report containing:
                1. ðŸ“‹ Problem Analysis Summary
                2. âœ… Correctness Verification
                3. âš¡ Performance Analysis
                4. ðŸŽ¯ Improvement Suggestions (Refactoring, Optimization)
                5. ðŸ“š Study Guide (Related concepts)
            """),
            agent=agent,
            context=context,
        )

    def kickoff(self) -> str:
        # Agents
        logic_agent = self.logic_expert()
        perf_agent = self.performance_specialist()
        quality_agent = self.code_quality_mentor()

        # Tasks
        logic_task = self.review_task(logic_agent, "Correctness, Logic, and Edge Cases")
        perf_task = self.review_task(
            perf_agent, "Time/Space Complexity and Optimizations"
        )
        quality_task = self.review_task(
            quality_agent, "Readability, Naming, and Best Practices"
        )

        # Final Synthesis Task
        # ì´ì „ íƒœìŠ¤í¬ë“¤ì˜ ê²°ê³¼(context)ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ìž‘ì„±í•©ë‹ˆë‹¤.
        final_task = self.report_aggregator_task(
            quality_agent, context=[logic_task, perf_task, quality_task]
        )

        crew = Crew(
            agents=[logic_agent, perf_agent, quality_agent],
            tasks=[logic_task, perf_task, quality_task, final_task],
            process=Process.sequential,
            verbose=True,
        )

        result = crew.kickoff()
        return str(result)


def run_algorithm_review(
    problem_info: str, solution_code: str, llm_config: LLMConfig
) -> str:
    crew_runner = AlgorithmReviewCrew(problem_info, solution_code, llm_config)
    return crew_runner.kickoff()
