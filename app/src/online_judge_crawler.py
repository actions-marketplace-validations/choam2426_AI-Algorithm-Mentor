import re
import time
from dataclasses import dataclass, field
from typing import Dict, List, Optional
from urllib.parse import urlparse

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait


@dataclass
class ProblemInfo:
    """ë¬¸ì œ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""

    site: str
    problem_id: str
    title: str
    difficulty: Optional[str] = None
    description: Optional[str] = None
    input_format: Optional[str] = None
    output_format: Optional[str] = None
    examples: List[Dict[str, str]] = field(default_factory=list)
    time_limit: Optional[str] = None
    memory_limit: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    url: str = None
    success: bool = True
    error_message: Optional[str] = None


class MultiSiteSeleniumCrawler:
    """ì…€ë ˆë‹ˆì›€ ê¸°ë°˜ 5ëŒ€ ì˜¨ë¼ì¸ ì €ì§€ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬"""

    # ì§€ì› ì‚¬ì´íŠ¸ ëª©ë¡
    SUPPORTED_SITES = {
        "acmicpc.net": "Baekjoon",
        "programmers.co.kr": "Programmers",
        "leetcode.com": "LeetCode",
        "codeforces.com": "Codeforces",
        "hackerrank.com": "HackerRank",
    }

    def __init__(self, headless=True, timeout=15):
        self.timeout = timeout
        self.headless = headless
        self.driver = None
        self.wait = None
        self._setup_driver()

    def _setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()

        if self.headless:
            chrome_options.add_argument("--headless=new")  # ìƒˆë¡œìš´ headless ëª¨ë“œ

        # ì„±ëŠ¥ ìµœì í™” ì˜µì…˜ë“¤
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument(
            "--disable-images"
        )  # ì´ë¯¸ì§€ ë¡œë”© ì°¨ë‹¨ìœ¼ë¡œ ì†ë„ í–¥ìƒ
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
        chrome_options.add_argument("--memory-pressure-off")
        chrome_options.add_argument("--max_old_space_size=4096")

        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(self.timeout)
            self.wait = WebDriverWait(self.driver, self.timeout)
            print("âœ… Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            raise Exception(
                f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}\nğŸ’¡ chromedriverê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
            )

    def get_site_name(self, url: str) -> str:
        """URLì—ì„œ ì‚¬ì´íŠ¸ëª… ì¶”ì¶œ"""
        try:
            domain = urlparse(url).netloc.lower()
            # www. ì œê±°
            if domain.startswith("www."):
                domain = domain[4:]

            for site_domain, site_name in self.SUPPORTED_SITES.items():
                if site_domain in domain:
                    return site_name

            return domain
        except:
            return "Unknown"

    def is_supported_site(self, url: str) -> bool:
        """ì§€ì›í•˜ëŠ” ì‚¬ì´íŠ¸ì¸ì§€ í™•ì¸"""
        domain = urlparse(url).netloc.lower()
        if domain.startswith("www."):
            domain = domain[4:]

        return any(site_domain in domain for site_domain in self.SUPPORTED_SITES.keys())

    def crawl_problem(self, url: str) -> ProblemInfo:
        """URLì—ì„œ ë¬¸ì œ ì •ë³´ í¬ë¡¤ë§"""
        if not self.is_supported_site(url):
            return ProblemInfo(
                site="Unknown",
                problem_id="unknown",
                title="ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸",
                url=url,
                success=False,
                error_message=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤. ì§€ì› ì‚¬ì´íŠ¸: {', '.join(self.SUPPORTED_SITES.values())}",
            )

        site_name = self.get_site_name(url)
        print(f"ğŸ” {site_name} í¬ë¡¤ë§ ì‹œì‘: {url}")

        try:
            if "acmicpc.net" in url:
                return self._crawl_baekjoon(url)
            elif "programmers.co.kr" in url:
                return self._crawl_programmers(url)
            elif "leetcode.com" in url:
                return self._crawl_leetcode(url)
            elif "codeforces.com" in url:
                return self._crawl_codeforces(url)
            elif "hackerrank.com" in url:
                return self._crawl_hackerrank(url)
            else:
                return ProblemInfo(
                    site=site_name,
                    problem_id="unknown",
                    title="ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸",
                    url=url,
                    success=False,
                    error_message="í•´ë‹¹ ì‚¬ì´íŠ¸ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                )

        except Exception as e:
            return ProblemInfo(
                site=site_name,
                problem_id="error",
                title="í¬ë¡¤ë§ ì‹¤íŒ¨",
                url=url,
                success=False,
                error_message=f"{site_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            )

    def _crawl_baekjoon(self, url: str) -> ProblemInfo:
        """ë°±ì¤€ ë¬¸ì œ í¬ë¡¤ë§"""
        self.driver.get(url)

        # ë¬¸ì œ ë²ˆí˜¸ ì¶”ì¶œ
        problem_id = re.search(r"/problem/(\d+)", url).group(1)

        # ë¬¸ì œ ì œëª©
        title = self.wait.until(
            EC.presence_of_element_located((By.ID, "problem_title"))
        ).text.strip()

        # ë¬¸ì œ ì„¤ëª…
        try:
            description_elem = self.driver.find_element(By.ID, "problem_description")
            description = description_elem.text.strip()
        except:
            description = None

        # ì…ì¶œë ¥ ì„¤ëª…
        try:
            input_elem = self.driver.find_element(By.ID, "problem_input")
            input_format = input_elem.text.strip()
        except:
            input_format = None

        try:
            output_elem = self.driver.find_element(By.ID, "problem_output")
            output_format = output_elem.text.strip()
        except:
            output_format = None

        # ì˜ˆì œ ì…ì¶œë ¥
        examples = []
        try:
            sample_inputs = self.driver.find_elements(
                By.CSS_SELECTOR, "pre[id^='sample-input-']"
            )
            sample_outputs = self.driver.find_elements(
                By.CSS_SELECTOR, "pre[id^='sample-output-']"
            )

            for inp, out in zip(sample_inputs, sample_outputs):
                examples.append({"input": inp.text.strip(), "output": out.text.strip()})
        except:
            pass

        # ì‹œê°„/ë©”ëª¨ë¦¬ ì œí•œ
        time_limit = memory_limit = None
        try:
            info_table = self.driver.find_element(By.ID, "problem-info")
            rows = info_table.find_elements(By.TAG_NAME, "tr")
            for row in rows:
                cells = row.find_elements(By.TAG_NAME, "td")
                if len(cells) >= 2:
                    cell_text = cells[0].text.strip()
                    if "ì‹œê°„" in cell_text:
                        time_limit = cells[1].text.strip()
                    elif "ë©”ëª¨ë¦¬" in cell_text:
                        memory_limit = cells[1].text.strip()
        except:
            pass

        return ProblemInfo(
            site="Baekjoon",
            problem_id=problem_id,
            title=title,
            description=description,
            input_format=input_format,
            output_format=output_format,
            examples=examples,
            time_limit=time_limit,
            memory_limit=memory_limit,
            url=url,
        )

    def _crawl_programmers(self, url: str) -> ProblemInfo:
        """í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ë¬¸ì œ í¬ë¡¤ë§"""
        self.driver.get(url)
        time.sleep(5)  # JavaScript ë¡œë”© ëŒ€ê¸°

        # ë¬¸ì œ ID ì¶”ì¶œ
        problem_id = re.search(r"/lessons/(\d+)", url)
        problem_id = problem_id.group(1) if problem_id else "unknown"

        # ë¬¸ì œ ì œëª© - ì—¬ëŸ¬ ì„ íƒì ì‹œë„
        title = "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
        title_selectors = [
            "h3.lesson-title",
            ".algorithm-title",
            "[data-cy='algorithm-title']",
            ".title",
            "h1",
            "h2",
            "h3",
            ".challenge-title",
        ]

        for selector in title_selectors:
            try:
                title_elem = self.wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                potential_title = title_elem.text.strip()
                if (
                    potential_title
                    and not potential_title.startswith("í”„ë¡œê·¸ë˜ë¨¸ìŠ¤")
                    and len(potential_title) > 3
                ):
                    title = potential_title
                    break
            except:
                continue

        # ë‚œì´ë„ ì¶”ì¶œ
        difficulty = None
        difficulty_selectors = [
            ".algorithm-level",
            ".level",
            "[class*='level']",
            ".difficulty",
            "[data-testid='level']",
        ]

        for selector in difficulty_selectors:
            try:
                diff_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                difficulty = diff_elem.text.strip()
                if difficulty and "Level" in difficulty:
                    break
            except:
                continue

        # ë¬¸ì œ ì„¤ëª… - ë¶€ë¶„ì ìœ¼ë¡œë§Œ ì¶”ì¶œ ê°€ëŠ¥
        description = None
        desc_selectors = [
            ".guide-section-description",
            ".algorithm-description",
            ".lesson-content",
            "[data-cy='algorithm-description']",
            ".content",
            ".markdown",
        ]

        for selector in desc_selectors:
            try:
                desc_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                desc_text = desc_elem.text.strip()
                if desc_text and len(desc_text) > 10:
                    description = desc_text[:800]  # ì²˜ìŒ 800ìë§Œ
                    break
            except:
                continue

        return ProblemInfo(
            site="Programmers",
            problem_id=problem_id,
            title=title,
            difficulty=difficulty,
            description=description,
            url=url,
        )

    def _crawl_leetcode(self, url: str) -> ProblemInfo:
        """ë¦¬íŠ¸ì½”ë“œ ë¬¸ì œ í¬ë¡¤ë§"""
        self.driver.get(url)
        time.sleep(8)  # GraphQL ë° React ë¡œë”© ëŒ€ê¸°

        # ë¬¸ì œ ID ì¶”ì¶œ
        problem_match = re.search(r"/problems/([^/]+)", url)
        problem_id = problem_match.group(1) if problem_match else "unknown"

        # ë¬¸ì œ ì œëª©
        title = "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
        title_selectors = [
            "[data-cy='question-title']",
            ".text-title-large",
            ".mr-2",
            "h1",
            ".css-v3d350",
            ".question-title",
        ]

        for selector in title_selectors:
            try:
                title_elem = self.wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                potential_title = title_elem.text.strip()
                if (
                    potential_title
                    and not potential_title.startswith("Sign")
                    and len(potential_title) > 3
                ):
                    # ë¬¸ì œ ë²ˆí˜¸ê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
                    clean_title = re.sub(r"^\d+\.\s*", "", potential_title)
                    title = clean_title if clean_title else potential_title
                    break
            except:
                continue

        # ë‚œì´ë„
        difficulty = None
        diff_selectors = [
            ".text-olive",  # Easy
            ".text-yellow",  # Medium
            ".text-pink",  # Hard
            "[diff='1']",
            "[diff='2']",
            "[diff='3']",
            ".difficulty",
        ]

        for selector in diff_selectors:
            try:
                diff_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                if "olive" in selector:
                    difficulty = "Easy"
                elif "yellow" in selector:
                    difficulty = "Medium"
                elif "pink" in selector:
                    difficulty = "Hard"
                else:
                    difficulty = diff_elem.text.strip()

                if difficulty:
                    break
            except:
                continue

        # ë¬¸ì œ ì„¤ëª… (ì œí•œì )
        description = None
        desc_selectors = [
            ".question-content",
            ".content__u3I1",
            "[data-track-load='description_content']",
            ".elfjS",
        ]

        for selector in desc_selectors:
            try:
                desc_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                desc_text = desc_elem.text.strip()
                if desc_text and len(desc_text) > 20:
                    description = desc_text[:1000]  # ì²˜ìŒ 1000ìë§Œ
                    break
            except:
                continue

        if not description:
            description = "ë¡œê·¸ì¸ì´ í•„ìš”í•˜ê±°ë‚˜ JavaScript ë Œë”ë§ ì§€ì—°ìœ¼ë¡œ ì¸í•´ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì œí•œ"

        return ProblemInfo(
            site="LeetCode",
            problem_id=problem_id,
            title=title,
            difficulty=difficulty,
            description=description,
            url=url,
        )

    def _crawl_codeforces(self, url: str) -> ProblemInfo:
        """ì½”ë“œí¬ìŠ¤ ë¬¸ì œ í¬ë¡¤ë§"""
        self.driver.get(url)

        # ë¬¸ì œ ID ì¶”ì¶œ
        problem_match = re.search(r"/problem/([^/]+/[^/]+)", url)
        if not problem_match:
            problem_match = re.search(r"/contest/(\d+)/problem/([A-Z]+)", url)
            problem_id = (
                f"{problem_match.group(1)}{problem_match.group(2)}"
                if problem_match
                else "unknown"
            )
        else:
            problem_id = problem_match.group(1)

        # ë¬¸ì œ ì œëª©
        title = self.wait.until(
            EC.presence_of_element_located(
                (By.CSS_SELECTOR, ".problem-statement .title")
            )
        ).text.strip()

        # ì‹œê°„/ë©”ëª¨ë¦¬ ì œí•œ
        time_limit = memory_limit = None
        try:
            time_elem = self.driver.find_element(By.CSS_SELECTOR, ".time-limit")
            time_limit = time_elem.text.strip()
        except:
            pass

        try:
            memory_elem = self.driver.find_element(By.CSS_SELECTOR, ".memory-limit")
            memory_limit = memory_elem.text.strip()
        except:
            pass

        # ë¬¸ì œ ì„¤ëª…
        description = None
        try:
            desc_elem = self.driver.find_element(By.CSS_SELECTOR, ".problem-statement")
            paragraphs = desc_elem.find_elements(By.TAG_NAME, "p")
            if paragraphs:
                description = paragraphs[0].text.strip()[
                    :800
                ]  # ì²« ë²ˆì§¸ ë¬¸ë‹¨, 800ì ì œí•œ
        except:
            pass

        # ì˜ˆì œ ì…ì¶œë ¥
        examples = []
        try:
            input_elems = self.driver.find_elements(By.CSS_SELECTOR, ".input pre")
            output_elems = self.driver.find_elements(By.CSS_SELECTOR, ".output pre")

            for inp, out in zip(input_elems, output_elems):
                examples.append({"input": inp.text.strip(), "output": out.text.strip()})
        except:
            pass

        return ProblemInfo(
            site="Codeforces",
            problem_id=problem_id,
            title=title,
            description=description,
            examples=examples,
            time_limit=time_limit,
            memory_limit=memory_limit,
            url=url,
        )

    def _crawl_hackerrank(self, url: str) -> ProblemInfo:
        """í•´ì»¤ë­í¬ ë¬¸ì œ í¬ë¡¤ë§"""
        self.driver.get(url)
        time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        # ë¬¸ì œ ID ì¶”ì¶œ
        problem_match = re.search(r"/challenges/([^/]+)", url)
        problem_id = problem_match.group(1) if problem_match else "unknown"

        # ë¬¸ì œ ì œëª©
        title = "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
        title_selectors = [
            ".challenge-name",
            ".challenge-title",
            "h1.ui-icon-label",
            ".page-header-text",
            "h1",
        ]

        for selector in title_selectors:
            try:
                title_elem = self.wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                potential_title = title_elem.text.strip()
                if potential_title and len(potential_title) > 3:
                    title = potential_title
                    break
            except:
                continue

        # ë‚œì´ë„
        difficulty = None
        diff_selectors = [
            ".difficulty",
            ".challenge-difficulty",
            "[class*='difficulty']",
        ]

        for selector in diff_selectors:
            try:
                diff_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                difficulty = diff_elem.text.strip()
                if difficulty:
                    break
            except:
                continue

        # ë¬¸ì œ ì„¤ëª… (ì œí•œì )
        description = None
        desc_selectors = [
            ".challenge-text",
            ".problem-statement",
            ".challenge-body-html",
            ".content",
        ]

        for selector in desc_selectors:
            try:
                desc_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                desc_text = desc_elem.text.strip()
                if desc_text and len(desc_text) > 20:
                    description = desc_text[:800]  # ì²˜ìŒ 800ìë§Œ
                    break
            except:
                continue

        return ProblemInfo(
            site="HackerRank",
            problem_id=problem_id,
            title=title,
            difficulty=difficulty,
            description=description,
            url=url,
        )

    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”´ Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


# URL ì¶”ì¶œ í•¨ìˆ˜ (ì´ì „ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
def extract_online_judge_urls(text: str) -> Dict[str, List[str]]:
    """ë¬¸ì¥ì—ì„œ ì˜¨ë¼ì¸ ì €ì§€ URLì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    import re

    # ì˜¨ë¼ì¸ ì €ì§€ ì‚¬ì´íŠ¸ ë„ë©”ì¸ ì •ì˜
    ONLINE_JUDGE_DOMAINS = {
        "acmicpc.net": "Baekjoon",
        "www.acmicpc.net": "Baekjoon",
        "programmers.co.kr": "Programmers",
        "school.programmers.co.kr": "Programmers",
        "leetcode.com": "LeetCode",
        "www.leetcode.com": "LeetCode",
        "codeforces.com": "Codeforces",
        "www.codeforces.com": "Codeforces",
        "hackerrank.com": "HackerRank",
        "www.hackerrank.com": "HackerRank",
    }

    # URL íŒ¨í„´ ì •ê·œì‹
    url_pattern = (
        r'https?://[^\s<>"{}|\\^`\[\]]+|www\.[^\s<>"{}|\\^`\[\]]+\.[a-zA-Z]{2,}'
    )

    # ëª¨ë“  URL ì°¾ê¸°
    urls = re.findall(url_pattern, text)
    urls = list(set(urls))  # ì¤‘ë³µ ì œê±°

    online_judge_urls = []
    online_judge_sites = []

    for url in urls:
        if url.startswith("www."):
            url = "http://" + url

        try:
            from urllib.parse import urlparse

            parsed_url = urlparse(url)
            domain = parsed_url.netloc.lower()

            if domain in ONLINE_JUDGE_DOMAINS:
                online_judge_urls.append(url)
                online_judge_sites.append(ONLINE_JUDGE_DOMAINS[domain])
        except:
            continue

    return {
        "all_urls": urls,
        "online_judge_urls": online_judge_urls,
        "online_judge_sites": online_judge_sites,
    }


# í¸ì˜ í•¨ìˆ˜ë“¤
def crawl_problems_from_text(text: str, headless: bool = True) -> List[ProblemInfo]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì˜¨ë¼ì¸ ì €ì§€ URLì„ ì°¾ì•„ í¬ë¡¤ë§"""
    url_info = extract_online_judge_urls(text)
    urls = url_info["online_judge_urls"]

    if not urls:
        print("âŒ í…ìŠ¤íŠ¸ì—ì„œ ì˜¨ë¼ì¸ ì €ì§€ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    print(f"ğŸ¯ {len(urls)}ê°œì˜ ì˜¨ë¼ì¸ ì €ì§€ URL ë°œê²¬")

    results = []
    with MultiSiteSeleniumCrawler(headless=headless) as crawler:
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] í¬ë¡¤ë§ ì¤‘...")
            problem = crawler.crawl_problem(url)
            results.append(problem)

            if problem.success:
                print(f"âœ… ì„±ê³µ: {problem.site} - {problem.title}")
            else:
                print(f"âŒ ì‹¤íŒ¨: {problem.error_message}")

            # ë‹¤ìŒ ìš”ì²­ ì „ ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i < len(urls):
                time.sleep(2)

    return results


def print_problem_details(problems: List[ProblemInfo]):
    """ë¬¸ì œ ì •ë³´ë“¤ì„ ìƒì„¸íˆ ì¶œë ¥"""
    if not problems:
        print("ğŸ“ í¬ë¡¤ë§ëœ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\n{'=' * 80}")
    print(f"ğŸ¯ ì´ {len(problems)}ê°œ ë¬¸ì œ í¬ë¡¤ë§ ê²°ê³¼")
    print(f"{'=' * 80}")

    # í†µê³„ ê³„ì‚°
    success_count = sum(1 for p in problems if p.success)
    site_stats = {}

    for problem in problems:
        site_stats[problem.site] = site_stats.get(problem.site, 0) + 1

    print(f"ğŸ“Š ì„±ê³µ: {success_count}ê°œ | ì‹¤íŒ¨: {len(problems) - success_count}ê°œ")
    print(f"ğŸ“ˆ ì‚¬ì´íŠ¸ë³„ í†µê³„: {dict(site_stats)}")

    # ê°œë³„ ë¬¸ì œ ìƒì„¸ ì •ë³´
    for i, problem in enumerate(problems, 1):
        print(f"\n{'â”€' * 60}")
        print(f"ğŸ”¢ ë¬¸ì œ {i}: {problem.site} - {problem.problem_id}")
        print(f"ğŸ“ ì œëª©: {problem.title}")
        print(f"ğŸŒ URL: {problem.url}")

        if problem.success:
            if problem.difficulty:
                print(f"âš¡ ë‚œì´ë„: {problem.difficulty}")

            if problem.time_limit:
                print(f"â° ì‹œê°„ ì œí•œ: {problem.time_limit}")

            if problem.memory_limit:
                print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì œí•œ: {problem.memory_limit}")

            if problem.description:
                desc = (
                    problem.description[:200] + "..."
                    if len(problem.description) > 200
                    else problem.description
                )
                print(f"ğŸ“– ì„¤ëª…: {desc}")

            if problem.examples:
                print(f"ğŸ“‹ ì˜ˆì œ ê°œìˆ˜: {len(problem.examples)}ê°œ")
                # ì²« ë²ˆì§¸ ì˜ˆì œë§Œ ì¶œë ¥
                if problem.examples:
                    ex = problem.examples[0]
                    print(
                        f"   ì…ë ¥: {ex['input'][:50]}{'...' if len(ex['input']) > 50 else ''}"
                    )
                    print(
                        f"   ì¶œë ¥: {ex['output'][:50]}{'...' if len(ex['output']) > 50 else ''}"
                    )
        else:
            print(f"âŒ ì˜¤ë¥˜: {problem.error_message}")


def format_problem_for_llm(
    problem: ProblemInfo,
    include_examples: bool = True,
    max_description_length: int = 1000,
) -> str:
    """
    ProblemInfoë¥¼ LLMì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜

    Args:
        problem: ProblemInfo ê°ì²´
        include_examples: ì˜ˆì œ ì…ì¶œë ¥ í¬í•¨ ì—¬ë¶€
        max_description_length: ì„¤ëª… ìµœëŒ€ ê¸¸ì´ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦¼)

    Returns:
        str: LLM ì¹œí™”ì ì¸ í˜•íƒœë¡œ í¬ë§·ëœ ë¬¸ìì—´
    """

    if not problem.success:
        return f"""
âŒ PROBLEM EXTRACTION FAILED
Site: {problem.site}
URL: {problem.url}
Error: {problem.error_message}
"""

    lines = []

    # í—¤ë” ì„¹ì…˜
    header = "ğŸ“š PROBLEM INFORMATION"
    lines.append(header)
    lines.append("=" * len(header))
    lines.append("")

    # ê¸°ë³¸ ì •ë³´
    lines.append(f"ğŸŒ Site: {problem.site}")
    lines.append(f"ğŸ”¢ Problem ID: {problem.problem_id}")
    lines.append(f"ğŸ“ Title: {problem.title}")

    if problem.difficulty:
        lines.append(f"âš¡ Difficulty: {problem.difficulty}")

    if problem.url:
        lines.append(f"ğŸ”— URL: {problem.url}")

    lines.append("")

    # ì œí•œ ì‚¬í•­
    if problem.time_limit or problem.memory_limit:
        lines.append("â±ï¸ CONSTRAINTS")
        lines.append("-" * 12)

        if problem.time_limit:
            lines.append(f"â€¢ Time Limit: {problem.time_limit}")

        if problem.memory_limit:
            lines.append(f"â€¢ Memory Limit: {problem.memory_limit}")

        lines.append("")

    # íƒœê·¸
    if problem.tags:
        lines.append("ğŸ·ï¸ TAGS")
        lines.append("-" * 6)
        tags_str = ", ".join(problem.tags)
        lines.append(f"â€¢ {tags_str}")
        lines.append("")

    # ë¬¸ì œ ì„¤ëª…
    if problem.description:
        lines.append("ğŸ“– PROBLEM DESCRIPTION")
        lines.append("-" * 20)

        # ì„¤ëª…ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        description = problem.description
        if len(description) > max_description_length:
            description = description[:max_description_length] + "... [truncated]"

        # ê¸´ ì„¤ëª…ì„ ì ì ˆíˆ ì¤„ë°”ê¿ˆ
        lines.append(description)
        lines.append("")

    # ì…ë ¥ í˜•ì‹
    if problem.input_format:
        lines.append("ğŸ“¥ INPUT FORMAT")
        lines.append("-" * 14)
        lines.append(problem.input_format)
        lines.append("")

    # ì¶œë ¥ í˜•ì‹
    if problem.output_format:
        lines.append("ğŸ“¤ OUTPUT FORMAT")
        lines.append("-" * 15)
        lines.append(problem.output_format)
        lines.append("")

    # ì˜ˆì œ
    if include_examples and problem.examples:
        lines.append("ğŸ’¡ EXAMPLES")
        lines.append("-" * 10)

        for i, example in enumerate(problem.examples, 1):
            lines.append(f"Example {i}:")
            lines.append("  Input:")
            # ì…ë ¥ì´ ì—¬ëŸ¬ ì¤„ì¸ ê²½ìš° ë“¤ì—¬ì“°ê¸°
            input_lines = example.get("input", "").strip().split("\n")
            for input_line in input_lines:
                lines.append(f"    {input_line}")

            lines.append("  Output:")
            # ì¶œë ¥ì´ ì—¬ëŸ¬ ì¤„ì¸ ê²½ìš° ë“¤ì—¬ì“°ê¸°
            output_lines = example.get("output", "").strip().split("\n")
            for output_line in output_lines:
                lines.append(f"    {output_line}")

            lines.append("")

    return "\n".join(lines)


# í…ŒìŠ¤íŠ¸ ë° ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ - 5ê°œ ì‚¬ì´íŠ¸ ëª¨ë‘ í¬í•¨
    test_text = """
    5ëŒ€ ì˜¨ë¼ì¸ ì €ì§€ ì‚¬ì´íŠ¸ ë¬¸ì œë“¤ì„ í…ŒìŠ¤íŠ¸í•´ë³´ê² ìŠµë‹ˆë‹¤:
    
    1. Baekjoon: https://www.acmicpc.net/problem/1000
    2. Programmers: https://programmers.co.kr/learn/courses/30/lessons/42576
    3. LeetCode: https://leetcode.com/problems/two-sum/
    4. Codeforces: https://codeforces.com/contest/1/problem/A
    5. HackerRank: https://www.hackerrank.com/challenges/solve-me-first/problem
    """

    print("ğŸš€ ì…€ë ˆë‹ˆì›€ ê¸°ë°˜ 5ëŒ€ ì˜¨ë¼ì¸ ì €ì§€ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    print(
        f"ì§€ì› ì‚¬ì´íŠ¸: {', '.join(MultiSiteSeleniumCrawler.SUPPORTED_SITES.values())}"
    )

    # í¬ë¡¤ë§ ì‹¤í–‰
    problems = crawl_problems_from_text(test_text, headless=True)

    # ê²°ê³¼ ì¶œë ¥
    print_problem_details(problems)

    # ì„±ê³µí•œ ë¬¸ì œë“¤ë§Œ ë”°ë¡œ ì¶œë ¥
    successful_problems = [p for p in problems if p.success]
    if successful_problems:
        print(f"\nğŸ‰ ì„±ê³µì ìœ¼ë¡œ í¬ë¡¤ë§ëœ {len(successful_problems)}ê°œ ë¬¸ì œ:")
        for problem in successful_problems:
            print(f"  âœ… {problem.site}: {problem.title}")

    print("\nğŸ’¡ ì‚¬ìš©ë²•:")
    print("1. í…ìŠ¤íŠ¸ì—ì„œ ìë™ ì¶”ì¶œ: crawl_problems_from_text('ë°±ì¤€ 1000ë²ˆ ë¬¸ì œ...')")
    print("2. ê°œë³„ URL í¬ë¡¤ë§:")
    print("   with MultiSiteSeleniumCrawler() as crawler:")
    print(
        "       problem = crawler.crawl_problem('https://www.acmicpc.net/problem/1000')"
    )

    print("\nğŸ“‹ í•„ìš”í•œ ì„¤ì¹˜:")
    print("pip install selenium beautifulsoup4")
    print("Chrome ë¸Œë¼ìš°ì € ë° chromedriver ì„¤ì¹˜ í•„ìš”")

    print("\nâš ï¸ ì£¼ì˜ì‚¬í•­:")
    print("- ê° ì‚¬ì´íŠ¸ì˜ ì´ìš©ì•½ê´€ì„ í™•ì¸í•˜ì„¸ìš”")
    print("- ê³¼ë„í•œ ìš”ì²­ìœ¼ë¡œ ì¸í•œ IP ì°¨ë‹¨ì— ì£¼ì˜í•˜ì„¸ìš”")
    print("- ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” ë¡œê·¸ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤


def crawl_single_problem(
    url: str, headless: bool = True, show_details: bool = True
) -> ProblemInfo:
    """ë‹¨ì¼ ë¬¸ì œ URL í¬ë¡¤ë§"""
    with MultiSiteSeleniumCrawler(headless=headless) as crawler:
        problem = crawler.crawl_problem(url)

        if show_details:
            print_problem_details([problem])

        return problem


def get_supported_sites() -> List[str]:
    """ì§€ì›í•˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡ ë°˜í™˜"""
    return list(MultiSiteSeleniumCrawler.SUPPORTED_SITES.values())


def batch_crawl_urls(
    urls: List[str], headless: bool = True, delay: int = 2
) -> List[ProblemInfo]:
    """URL ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ í¬ë¡¤ë§"""
    results = []

    print(f"ğŸ¯ {len(urls)}ê°œ URL ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘")

    with MultiSiteSeleniumCrawler(headless=headless) as crawler:
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] {url}")

            problem = crawler.crawl_problem(url)
            results.append(problem)

            if problem.success:
                print(f"âœ… {problem.site}: {problem.title}")
            else:
                print(f"âŒ ì‹¤íŒ¨: {problem.error_message}")

            # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
            if i < len(urls):
                time.sleep(delay)

    return results


def save_problems_to_json(problems: List[ProblemInfo], filename: str = "problems.json"):
    """ë¬¸ì œ ì •ë³´ë“¤ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    import json
    from dataclasses import asdict

    problems_data = []
    for problem in problems:
        problem_dict = asdict(problem)
        problems_data.append(problem_dict)

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(problems_data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ {len(problems)}ê°œ ë¬¸ì œ ì •ë³´ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def load_problems_from_json(filename: str = "problems.json") -> List[ProblemInfo]:
    """JSON íŒŒì¼ì—ì„œ ë¬¸ì œ ì •ë³´ë“¤ì„ ë¡œë“œ"""
    import json

    try:
        with open(filename, "r", encoding="utf-8") as f:
            problems_data = json.load(f)

        problems = []
        for problem_dict in problems_data:
            problem = ProblemInfo(**problem_dict)
            problems.append(problem)

        print(f"ğŸ“ {filename}ì—ì„œ {len(problems)}ê°œ ë¬¸ì œ ì •ë³´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return problems

    except FileNotFoundError:
        print(f"âŒ {filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        return []


# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def performance_test():
    """í¬ë¡¤ëŸ¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    test_urls = [
        "https://www.acmicpc.net/problem/1000",  # ë°±ì¤€ - ê°€ì¥ ì‰¬ìš´ ë¬¸ì œ
        "https://programmers.co.kr/learn/courses/30/lessons/42576",  # í”„ë¡œê·¸ë˜ë¨¸ìŠ¤
        "https://leetcode.com/problems/two-sum/",  # ë¦¬íŠ¸ì½”ë“œ
    ]

    print("âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    start_time = time.time()

    problems = batch_crawl_urls(test_urls, headless=True, delay=1)

    end_time = time.time()
    total_time = end_time - start_time

    success_count = sum(1 for p in problems if p.success)

    print("\nğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time / len(test_urls):.2f}ì´ˆ/ë¬¸ì œ")
    print(
        f"   ì„±ê³µë¥ : {success_count}/{len(test_urls)} ({success_count / len(test_urls) * 100:.1f}%)"
    )

    return problems


# ì—ëŸ¬ ë¶„ì„ í•¨ìˆ˜
def analyze_errors(problems: List[ProblemInfo]):
    """í¬ë¡¤ë§ ì—ëŸ¬ ë¶„ì„"""
    failed_problems = [p for p in problems if not p.success]

    if not failed_problems:
        print("ğŸ‰ ëª¨ë“  ë¬¸ì œê°€ ì„±ê³µì ìœ¼ë¡œ í¬ë¡¤ë§ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return

    print(f"\nğŸ” ì—ëŸ¬ ë¶„ì„ ({len(failed_problems)}ê°œ ì‹¤íŒ¨):")

    error_types = {}
    site_errors = {}

    for problem in failed_problems:
        # ì—ëŸ¬ íƒ€ì…ë³„ ë¶„ë¥˜
        error_msg = problem.error_message or "Unknown error"
        error_type = error_msg.split(":")[0] if ":" in error_msg else error_msg
        error_types[error_type] = error_types.get(error_type, 0) + 1

        # ì‚¬ì´íŠ¸ë³„ ì—ëŸ¬
        site_errors[problem.site] = site_errors.get(problem.site, 0) + 1

    print("ğŸ“ˆ ì—ëŸ¬ íƒ€ì…ë³„ í†µê³„:")
    for error_type, count in error_types.items():
        print(f"   {error_type}: {count}ê°œ")

    print("ğŸŒ ì‚¬ì´íŠ¸ë³„ ì‹¤íŒ¨ í†µê³„:")
    for site, count in site_errors.items():
        print(f"   {site}: {count}ê°œ")

    print("\nğŸ’¡ í•´ê²° ë°©ì•ˆ:")
    print("   - JavaScript ë¡œë”© ì‹œê°„ ë¶€ì¡±: time.sleep() ì¦ê°€")
    print("   - ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ í™•ì¸")
    print("   - ë¡œê·¸ì¸ í•„ìš”: ì¸ì¦ ê¸°ëŠ¥ ì¶”ê°€ ê³ ë ¤")
    print("   - ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ì¬ì‹œë„ ë¡œì§ êµ¬í˜„")


# ì‚¬ì´íŠ¸ë³„ ì„±ëŠ¥ ë¹„êµ
def compare_site_performance():
    """ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ì„±ëŠ¥ ë¹„êµ"""
    test_urls = {
        "Baekjoon": "https://www.acmicpc.net/problem/1000",
        "Programmers": "https://programmers.co.kr/learn/courses/30/lessons/42576",
        "LeetCode": "https://leetcode.com/problems/two-sum/",
        "Codeforces": "https://codeforces.com/contest/1/problem/A",
        "HackerRank": "https://www.hackerrank.com/challenges/solve-me-first/problem",
    }

    results = {}

    print("ğŸ† ì‚¬ì´íŠ¸ë³„ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("-" * 50)

    with MultiSiteSeleniumCrawler(headless=True) as crawler:
        for site_name, url in test_urls.items():
            print(f"\nğŸ” {site_name} í…ŒìŠ¤íŠ¸ ì¤‘...")

            start_time = time.time()
            problem = crawler.crawl_problem(url)
            end_time = time.time()

            duration = end_time - start_time
            results[site_name] = {
                "duration": duration,
                "success": problem.success,
                "title": problem.title if problem.success else "ì‹¤íŒ¨",
                "problem": problem,
            }

            status = "âœ…" if problem.success else "âŒ"
            print(
                f"{status} {site_name}: {duration:.2f}ì´ˆ - {results[site_name]['title']}"
            )

            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

    # ê²°ê³¼ ì •ë ¬ (ì„±ê³µí•œ ê²ƒë“¤ë§Œ, ì†ë„ ìˆœ)
    successful_results = {k: v for k, v in results.items() if v["success"]}
    sorted_results = sorted(successful_results.items(), key=lambda x: x[1]["duration"])

    print("\nğŸ¥‡ ì„±ëŠ¥ ìˆœìœ„ (ì„±ê³µí•œ ì‚¬ì´íŠ¸ë§Œ):")
    for i, (site_name, result) in enumerate(sorted_results, 1):
        print(f"   {i}ìœ„. {site_name}: {result['duration']:.2f}ì´ˆ")

    return results
